ML Notes: - https://chatgpt.com/share/245dbe82-3740-434d-82a6-c5ccbdec2881

Types of Weight Distribution techniques:

Here I/P and O/P means number of Inputs and Outputs

1) Uniform Distribution: Here value Wij where i is hidden layer and j is unit in hidden layer is selected from a uniform distribution whose min value is -1/root of I/P
and max is 1/ root of I/P.

2)Xavier/Glora Initialization:
  Two Types:  
            1) Xavier Normal initialization from normal distribution where Wij is equivalent to N(0,σ) where σ = Root of (2/input + output)
            2) Xavier Uniform also from normal distribution but range is min = - (root 6)/ root input+output max = root 6)/ root input+output

3) Kaiming He Initialization:
  Two Types:
            1) He Normal initializatio same as above mut different range  Wij is equivalent to N(0,σ) where σ = Root of (2/input)
            2) He Uniform same as above type but range is min = - (root 6)/ root input max = root 6)/ root input

Dropout:
Dropout is a regularization technique used in neural networks to prevent overfitting. Here's an overview of the key aspects of dropout:

Basic concept:
Randomly "drop out" (i.e., temporarily remove) a portion of neurons during training.

How it works:
During each training iteration, each neuron has a probability p of being temporarily disabled.
The network is forced to learn with a different subset of neurons each time.
(Or in training instead of disabling the neuron we can multiply its weight with p)

Implementation:
Training phase: Multiply each neuron's output by a binary mask (0 or 1) drawn from a Bernoulli distribution with probability p.
Inference phase: All neurons are active, but their outputs are scaled by p to maintain the expected output magnitude.

Benefits:
Reduces overfitting by preventing complex co-adaptations between neurons.
Acts as an ensemble method, effectively training many different networks.
Improves generalization by making the network more robust.



Convolution Neural Network:

It is used for image processing.

Image types:
Gray Scale Image(Black and white Image): has 1 channel and is represented Length * width * No. Of Channel = 6*6*1
RGB Image(Color Image): has 3 channel. Example 6*6*3

1) Step is to Normalize image to get its value between 0 to 1. Which can be done by dividing value of each pixel by 255 because range of pixel is 0 to 255.
2) Then we perform convulution operation which is the process of extracting features from images which done putting up a matrix know as filter(There can be of many types and its values are derived from back propagation and filter can be smaller than image or of same size. It helps extract feature like veritcal or horizontal or circular edges etc) on top of the image and in multiped and added to each of the scalar of image to create a new matrix. Stride is used to define the movement of the filter like after 1st output by how much pixel it should be shifted to in any direction .See video to understand more.
3)This new matrix is smaller than the image which results in information loss so we use padding.
  Two Types:
  1) Zero Padding: We assign zero to the new zone
  2) Neighbour Padding: We assign the value of the nearest pixel or neighbour pixel.
To define the size of padding formula is
n-f+2p+1=x
N is size of original image
f is size of frame
p is padding
x is the size of the required output size

4) After applying filters we get their respective matrices on which we apply relu activation function to assign weights effective during backward propagation.
5) All the process till now it is called Convolutional layer and their can be n number of convonutional layers.
6) Min, Average, Max pooling:
Pooling is a technique used to extract more feature from the resultant matrices of the filter.
Here a pooling matrix is created, which applied on the resultant matrix and it creates a new matrix and value of each scalar in new matrix is the max value of the transposed matrices the resultant matrix and pooling matrix then pooling matrix is shifted by stride.
Same for min and average

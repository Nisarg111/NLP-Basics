ML Notes: - https://chatgpt.com/share/245dbe82-3740-434d-82a6-c5ccbdec2881


Decision Tree:
Two Types:
ID3 can have max n number of child.
CART can have max 2 number of child.

Purity of a split is checked by:
1)Entropy
2)Gini Index

Information Gain is used to select the feature to make a split.


Entropy: 

Formula H(C1): -(P+ log2P+) - (P- log2P-) For Binary Classification. 
Range is 0 to 1 where 1 is highly impure and 0 is pure
P+ is the probability of output being positive
P- is the probability of output being negative.

Gini Impurity:

Formula: 1 - Sum of i to n P^2, where P = (P+^2 + P-^2)
Range is 0 to 0.5 where 0 is pure where 0.5 is impure


Gini Impurity:

Generally faster to compute
Works well with small datasets
Often preferred in random forest algorithms
Tends to isolate the most frequent class in its own branch

Entropy:

Can be more sensitive to impurities
May create more balanced trees
Often used in ID3 algorithm
Can be slightly slower to compute than Gini

In practice, the choice between Gini impurity and entropy often doesn't make a significant difference in the performance of the decision tree. Many data scientists experiment with both and choose the one that gives slightly better results for their specific dataset.

Information Gain:

Information Gain is used to find the best feature to split.

Formula: H(S) - Sum of ( (|Sv| / |S|) * H(Sv))

Here H(S) is the entropy of the root feature.
Then Next part represent the sum of 
|Sv| is the total number of splits of the child Like for a binary category we have 5yes/3 No then Sv = 8
|S| is the total number of splits in root
H(Sv) is the entropy of the child
We will calculate this for every child of root node and depth of the tree can be n i.e. tree can be classified or split to n depth.





Types of Weight Distribution techniques:

Here I/P and O/P means number of Inputs and Outputs

1) Uniform Distribution: Here value Wij where i is hidden layer and j is unit in hidden layer is selected from a uniform distribution whose min value is -1/root of I/P
and max is 1/ root of I/P.

2)Xavier/Glora Initialization:
  Two Types:  
            1) Xavier Normal initialization from normal distribution where Wij is equivalent to N(0,σ) where σ = Root of (2/input + output)
            2) Xavier Uniform also from normal distribution but range is min = - (root 6)/ root input+output max = root 6)/ root input+output

3) Kaiming He Initialization:
  Two Types:
            1) He Normal initializatio same as above mut different range  Wij is equivalent to N(0,σ) where σ = Root of (2/input)
            2) He Uniform same as above type but range is min = - (root 6)/ root input max = root 6)/ root input

Dropout:
Dropout is a regularization technique used in neural networks to prevent overfitting. Here's an overview of the key aspects of dropout:

Basic concept:
Randomly "drop out" (i.e., temporarily remove) a portion of neurons during training.

How it works:
During each training iteration, each neuron has a probability p of being temporarily disabled.
The network is forced to learn with a different subset of neurons each time.
(Or in training instead of disabling the neuron we can multiply its weight with p)

Implementation:
Training phase: Multiply each neuron's output by a binary mask (0 or 1) drawn from a Bernoulli distribution with probability p.
Inference phase: All neurons are active, but their outputs are scaled by p to maintain the expected output magnitude.

Benefits:
Reduces overfitting by preventing complex co-adaptations between neurons.
Acts as an ensemble method, effectively training many different networks.
Improves generalization by making the network more robust.



Convolution Neural Network:

It is used for image processing.

Image types:
Gray Scale Image(Black and white Image): has 1 channel and is represented Length * width * No. Of Channel = 6*6*1
RGB Image(Color Image): has 3 channel. Example 6*6*3

1) Step is to Normalize image to get its value between 0 to 1. Which can be done by dividing value of each pixel by 255 because range of pixel is 0 to 255.
2) Then we perform convulution operation which is the process of extracting features from images which done putting up a matrix know as filter(There can be of many types and its values are derived from back propagation and filter can be smaller than image or of same size. It helps extract feature like veritcal or horizontal or circular edges etc) on top of the image and in multiped and added to each of the scalar of image to create a new matrix. Stride is used to define the movement of the filter like after 1st output by how much pixel it should be shifted to in any direction .See video to understand more.
3)This new matrix is smaller than the image which results in information loss so we use padding.
  Two Types:
  1) Zero Padding: We assign zero to the new zone
  2) Neighbour Padding: We assign the value of the nearest pixel or neighbour pixel.
To define the size of padding formula is
n-f+2p+1=x
N is size of original image
f is size of frame
p is padding
x is the size of the required output size

4) After applying filters we get their respective matrices on which we apply relu activation function to assign weights effective during backward propagation.
5) All the process till now it is called Convolutional layer and their can be n number of convonutional layers.
6) Min, Average, Max pooling:
Pooling is a technique used to extract more feature from the resultant matrices of the filter.
Here a pooling matrix is created, which applied on the resultant matrix and it creates a new matrix and value of each scalar in new matrix is the max value of the transposed matrices the resultant matrix and pooling matrix then pooling matrix is shifted by stride.
Same for min and average. 

It is called Location invariant.

7)Flattening means to convert all matrices into a single column vector. All matrices are appended one after another.
8)This output is passed to Fully connected neural network.
9) From input to flatting is called Feature Extraction, Then FCNN is called classification.

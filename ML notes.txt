ML Notes: - https://chatgpt.com/share/245dbe82-3740-434d-82a6-c5ccbdec2881


Decision Tree:
Two Types:
ID3 can have max n number of child.
CART can have max 2 number of child.


Classification Problem:

Purity of a split is checked by:
1)Entropy
2)Gini Index

Information Gain is used to select the feature to make a split.


Entropy: 

Formula H(C1): -(P+ log2P+) - (P- log2P-) For Binary Classification. 
Range is 0 to 1 where 1 is highly impure and 0 is pure
P+ is the probability of output being positive
P- is the probability of output being negative.

Gini Impurity:

Formula: 1 - Sum of i to n P^2, where P = (P+^2 + P-^2)
Range is 0 to 0.5 where 0 is pure where 0.5 is impure


Gini Impurity:

Generally faster to compute
Works well with small datasets
Often preferred in random forest algorithms
Tends to isolate the most frequent class in its own branch

Entropy:

Can be more sensitive to impurities
May create more balanced trees
Often used in ID3 algorithm
Can be slightly slower to compute than Gini

In practice, the choice between Gini impurity and entropy often doesn't make a significant difference in the performance of the decision tree. Many data scientists experiment with both and choose the one that gives slightly better results for their specific dataset.

Information Gain:

Information Gain is used to find the best feature to split.

Formula: H(S) - Sum of ( (|Sv| / |S|) * H(Sv))

Here H(S) is the entropy of the root feature.
Then Next part represent the sum of 
|Sv| is the total number of splits of the child Like for a binary category we have 5yes/3 No then Sv = 8
|S| is the total number of splits in root
H(Sv) is the entropy of the child
We will calculate this for every child of root node and depth of the tree can be n i.e. tree can be classified or split to n depth.

Prunning:

Prunning is used to avoid overfitting of data which happens when we create a decision tree with all possible splits. It is also called Low Bais and High Variance.

Two Types:
Post Prunning: First we create Decision Tree, then apply prunning. Usefull for small dataset.
Pre Prunning: Hyperparameter tuning while constructing Decision Tree.
Some of the features: max depth, max feature, criterrian, splitter etc. 


Regression Problem:

We use variance to decide the split.

We will do it for each feature to decide and same as classification problem we will apply this to root and its child.
Variance(root) = 1/n (Sum of 1 to n (y - Ybar(Average Y))^2)
here n is no. of total element.
For example we feature Year and it has 3,4,5,6,7 and we make a split with year<=3 so on left we have 1 element and for right we 4 elements.
Taking this example, if root is 4 then n is 5.
For its child on left n = 1 and for right child n = 4

After we calculate this for all root feature we will apply Varience Reduction:
Formula = Var(root) - Sum of 1 to n(Wi * Var child)
Now Wi for above example we will be:
For left child only 1 element is from total 5 elements so its weight will be 1/5.
For right child it has 4 elements out of 5 so its weight will be 4/5.
Root node with high variance reduction is selected.


Types of Weight Distribution techniques:

Here I/P and O/P means number of Inputs and Outputs

1) Uniform Distribution: Here value Wij where i is hidden layer and j is unit in hidden layer is selected from a uniform distribution whose min value is -1/root of I/P
and max is 1/ root of I/P.

2)Xavier/Glora Initialization:
  Two Types:  
            1) Xavier Normal initialization from normal distribution where Wij is equivalent to N(0,σ) where σ = Root of (2/input + output)
            2) Xavier Uniform also from normal distribution but range is min = - (root 6)/ root input+output max = root 6)/ root input+output

3) Kaiming He Initialization:
  Two Types:
            1) He Normal initializatio same as above mut different range  Wij is equivalent to N(0,σ) where σ = Root of (2/input)
            2) He Uniform same as above type but range is min = - (root 6)/ root input max = root 6)/ root input

Dropout:
Dropout is a regularization technique used in neural networks to prevent overfitting. Here's an overview of the key aspects of dropout:

Basic concept:
Randomly "drop out" (i.e., temporarily remove) a portion of neurons during training.

How it works:
During each training iteration, each neuron has a probability p of being temporarily disabled.
The network is forced to learn with a different subset of neurons each time.
(Or in training instead of disabling the neuron we can multiply its weight with p)

Implementation:
Training phase: Multiply each neuron's output by a binary mask (0 or 1) drawn from a Bernoulli distribution with probability p.
Inference phase: All neurons are active, but their outputs are scaled by p to maintain the expected output magnitude.

Benefits:
Reduces overfitting by preventing complex co-adaptations between neurons.
Acts as an ensemble method, effectively training many different networks.
Improves generalization by making the network more robust.




Random Forest Classifier:

Ensemble Technique: Ensemble learning is a machine learning technique that aggregates two or more learners (e.g. regression models, neural networks) in order to produce better predictions.

Types:
1) Bagging: N number of model are trained on subset of dataset parallely and then their output is combined to get the output. For classification Major Voting(Class that is selected by most model) and for regression Average is taken of all output.Example
    Bagging Meta-estimator.
    Random Forest.
    Extra Trees (Extremely Randomized Trees)
    Pasting.
    Bootstrap Aggregating (Bagging)
    Balanced Random Forest.
    Random Subspaces.
    Feature Bagging.
2) Boosting: N number of model are trainned sequentially. To first model entire dataset is passed then it makes prediction, the prediction that are incorrect are passed to the next weak learner with the dataset these cycle is repeated till a strong learner is produced by combining all weak learner.


Random Forest:

For each model in decision tree we provide a subset of dataset which is done using row sampling and feature sampling which means we only pass selected rows and columns to the model. Two or model might have same features and row.

Classification: Major Voting
Regression: Average of Output of all model

Why use Random Forest instead of Decision Tree?
Because Decision Tree leads to overfitting i.e low bais and high vairance.




Column Transformer:
Column transformer is used to combine preprocessing of different types of data like for numeric standardscalar and for categorical one hot encoding.
Example:

preprocessor = ColumnTransformer([
("OneHotEncoder",name_of_transformer_obj,name_of_columns),
("StandardScalar",name_of_transformer_obj,name_of_columns)
])



SVM (Support Vector Machine):

https://chatgpt.com/share/33cc8a55-888a-4327-ba38-d6b01d1632cd

It has two Types:
1) Support Vector Regressor
2) Support Vector Classifier


Distance below the plane or angle with the best fit line and point > 90 degree, distance will be negative.
Distance above the plane or angle with the hyperplane and point is less than 90 degree, distance will be positive.
Formula for best fit line = W(transpose)ofx + b = 0 
Formula for positive support vector = W(transpose)ofx + b = +1
Formula for negative support vector = W(transpose)ofx + b = -1
Cost Function = Min(w,b) ||w||/2 + Ci Sum of i to n Eta of i      
(||w|| is called magnitude of w and cost function should be maximized to increase the distance between the support vector)
(Ci to i is Hingle loss and Ci represents how many no. of pointes can be misclassified and tolerated)
(Eta is represented with a curvy horizontal line and it is summation of distance of incorrect points from the marginal plane)

Regression PROBLEM:

Predicted and observed should in support marginal vector, this indicates model is performing well.
Points above or below those vectors are considered for calculation of hinge loss.
Here formular for best support vectors are W(transpose) of x + b + Epsilon for positive
Here formular for best support vectors are W(transpose) of x + b - Epsilon for negative
(Epsilon is error between the predicted points and the marginal vector)

Kernel are used when data points can not be separable in the given plain so we need to convert them into different plain, there transformation is done using kernel.
Examples of kernel:
1) Polynomial
2)linear
3)rbf
4)sigmoid


Convolution Neural Network:

It is used for image processing.

Image types:
Gray Scale Image(Black and white Image): has 1 channel and is represented Length * width * No. Of Channel = 6*6*1
RGB Image(Color Image): has 3 channel. Example 6*6*3

1) Step is to Normalize image to get its value between 0 to 1. Which can be done by dividing value of each pixel by 255 because range of pixel is 0 to 255.
2) Then we perform convulution operation which is the process of extracting features from images which done putting up a matrix know as filter(There can be of many types and its values are derived from back propagation and filter can be smaller than image or of same size. It helps extract feature like veritcal or horizontal or circular edges etc) on top of the image and in multiped and added to each of the scalar of image to create a new matrix. Stride is used to define the movement of the filter like after 1st output by how much pixel it should be shifted to in any direction .See video to understand more.
3)This new matrix is smaller than the image which results in information loss so we use padding.
  Two Types:
  1) Zero Padding: We assign zero to the new zone
  2) Neighbour Padding: We assign the value of the nearest pixel or neighbour pixel.
To define the size of padding formula is
n-f+2p+1=x
N is size of original image
f is size of frame
p is padding
x is the size of the required output size

4) After applying filters we get their respective matrices on which we apply relu activation function to assign weights effective during backward propagation.
5) All the process till now it is called Convolutional layer and their can be n number of convonutional layers.
6) Min, Average, Max pooling:
Pooling is a technique used to extract more feature from the resultant matrices of the filter.
Here a pooling matrix is created, which applied on the resultant matrix and it creates a new matrix and value of each scalar in new matrix is the max value of the transposed matrices the resultant matrix and pooling matrix then pooling matrix is shifted by stride.
Same for min and average. 

It is called Location invariant.

7)Flattening means to convert all matrices into a single column vector. All matrices are appended one after another.
8)This output is passed to Fully connected neural network.
9) From input to flatting is called Feature Extraction, Then FCNN is called classification.



Linearity vs non - linearity:

In machine learning (ML), the concepts of linearity and non-linearity are essential for understanding how models learn from data and make predictions. Let’s dive deeper into these concepts and explore them with examples.
1. Linearity in Machine Learning:

In machine learning, a model is considered linear if the output is a linear combination of the input features. This means that the relationship between the input features and the output can be represented as a straight line (or hyperplane in higher dimensions).
Linear Model Example:

Consider a simple linear regression model:

y=w1x1+w2x2+⋯+wnxn+by=w1​x1​+w2​x2​+⋯+wn​xn​+b

Here:

    yy is the predicted output.
    x1,x2,…,xnx1​,x2​,…,xn​ are the input features.
    w1,w2,…,wnw1​,w2​,…,wn​ are the weights or coefficients that the model learns during training.
    bb is the bias term.

Key Characteristics of Linear Models:

    The output is a weighted sum of the inputs.
    The model assumes a linear relationship between the inputs and the output.
    The decision boundary (in classification) or the prediction function (in regression) is a straight line or hyperplane.

Example: Predicting House Prices

Suppose you want to predict the price of a house based on its size and number of bedrooms. A linear regression model might look like this:

Price=50×Size+10000×Bedrooms+50000Price=50×Size+10000×Bedrooms+50000

Here:

    The coefficient 50 is the weight for the "Size" feature.
    The coefficient 10000 is the weight for the "Bedrooms" feature.
    The bias term 50000 represents the base price.

This model assumes that the price increases linearly with the size and number of bedrooms. If the relationship between house size, number of bedrooms, and price is actually more complex, this linear model may not capture the true patterns in the data well.
2. Non-Linearity in Machine Learning:

A model is considered non-linear if the relationship between the input features and the output cannot be represented as a straight line or hyperplane. Non-linear models can capture more complex patterns in the data, which is crucial for tasks where the relationship between features and output is not linear.
Non-Linear Model Example:

A non-linear model might involve polynomial terms, interactions between features, or non-linear activation functions in neural networks.
Example: Polynomial Regression

Let’s extend the linear regression example by adding a non-linear term:

y=w1x+w2x2+by=w1​x+w2​x2+b

This is a polynomial regression model of degree 2. The addition of the x2x2 term makes the model non-linear.

Key Characteristics of Non-Linear Models:

    The output is not a simple weighted sum of the inputs.
    The decision boundary or prediction function can be curved or more complex.
    Non-linear models can capture interactions between features and handle more complex data patterns.

Example: Predicting House Prices with Non-Linearity

Let’s say the relationship between house size and price is quadratic. The model might look like:

Price=30×Size+5×Size2+10000×Bedrooms+50000Price=30×Size+5×Size2+10000×Bedrooms+50000

Here:

    The 5×Size25×Size2 term introduces non-linearity, allowing the model to capture more complex relationships between house size and price.
    This non-linear model can better fit situations where, for example, the price increases at a faster rate as the house size increases.

3. Non-Linearity in Neural Networks:

In neural networks, non-linearity is introduced by using non-linear activation functions like ReLU, sigmoid, or tanh. Without these non-linearities, no matter how many layers a neural network has, it would behave like a linear model, which limits its ability to learn complex patterns.
Example: Neural Network with and without Non-Linearity

Consider a simple neural network with one hidden layer. The output yy is calculated as:

y=f(W2⋅f(W1⋅x+b1)+b2)y=f(W2​⋅f(W1​⋅x+b1​)+b2​)

Where:

    xx is the input vector.
    W1W1​ and W2W2​ are weight matrices.
    b1b1​ and b2b2​ are biases.
    f(⋅)f(⋅) is a non-linear activation function like ReLU.

Without Non-Linearity:

If f(⋅)f(⋅) were a linear function (e.g., identity function), the network would collapse into a single linear layer, making it equivalent to a linear model:

y=W′⋅x+b′y=W′⋅x+b′

This would limit the network's ability to model complex relationships.

With Non-Linearity:

By using a non-linear function like ReLU:

f(z)=max⁡(0,z)f(z)=max(0,z)

The network can model complex functions. Each layer applies a non-linear transformation, allowing the network to capture intricate patterns and interactions in the data.
Why Non-Linearity is Important in ML:

    Capturing Complex Relationships:
        Many real-world relationships between inputs and outputs are non-linear. For example, the effect of age on income, the relationship between temperature and ice cream sales, or the impact of multiple factors on disease diagnosis.
        Non-linear models can better fit such complex relationships.

    Decision Boundaries in Classification:
        In classification tasks, linear models create straight-line (or hyperplane) decision boundaries, which might not be sufficient to separate classes in complex datasets. Non-linear models can create curved or irregular decision boundaries that better separate the classes.

    Feature Interactions:
        Non-linear models can capture interactions between features, where the effect of one feature depends on the value of another. For example, the impact of exercise on health might depend on age.

    Versatility in Neural Networks:
        Neural networks rely on non-linear activation functions to learn from data. Without these non-linearities, deep networks would lose their ability to model anything more than simple linear transformations.

Summary:

    Linear Models assume a straightforward, proportional relationship between inputs and outputs. They are easy to interpret and fast to train but might be too simplistic for complex tasks.

    Non-Linear Models introduce complexity by allowing the model to capture intricate patterns, relationships, and interactions within the data. This makes them more powerful for a wide range of tasks, especially those where the true relationship between inputs and outputs is not linear.

Understanding when to use linear versus non-linear models is key to building effective machine learning systems.






K Means Clustering:

Here, we have to create a clusters for given data points. It is done by first randomly initializing k centroids and labeling the data points near to the centroid of their own type.
After all data points are labelled, centroid position is changed to the mean pos of all data points of its type and this process is repeated.

Problem with this is Random initializtion Trap, if two or more centroids are placed near to each other than cluster being formed is not correct.

To overcome this we plot cenrtroids randomly in such a way that they are max far from each other.

And way of k is determined by Elbow method in graph where K and WCSS(Within Cluster Sum of Squares)
WCSS = Sum of i to n points from k

We plot a graph using this k being on X axis and WCSS on the Y axis.
As the k will increase and WCSS will decrease and at some point it will become flat and we get the point before the graphs get flatten.

To verify our clusters we use Silhoute Score.

Its calculation:

We take a point i from a cluster and we calculate it average from the rest of the points in  the cluster it is done by summation of distance from all points and then dividing it with (Total number of points in cluster - 1) a(i)

Then we take the same point and do the above steps with other nearest cluster.b(i)
Then we calculate Silhoute score which is [a(i)-b(i)/[max[a(i),b(i)]

Value of silhoute score is between -1 to 1.

And more near to 1 better the clustering is.
If equal to 0 then a(i) = b(i)




Dimensionality Reduction:

Curse of Dimensionality: It occurs when we overload the model with n number of features that the accuracy of the model becomes to degrade.

To overcome Curse of Dimensionality we use Dimensionality reduction. This can be done in two ways:
1) Feature Selection : Here we select and give features to model that are releavant and useful for the model and problem statement.
2) Feature Extraction : Here we try to capture the data from multiple features to a single feature.

Use case of Dimensionality Reduction:

1) Easy to visualize data
2) Improved model performance




PCA (Princial Component Analysis) Feature Extraction:

It creates new axises ( Views ) which captures the spread and variance which is less than the original axis hence providing better feature.
Here goal is to capture maximum variance

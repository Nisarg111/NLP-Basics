ML Notes: - https://chatgpt.com/share/245dbe82-3740-434d-82a6-c5ccbdec2881


Decision Tree:
Two Types:
ID3 can have max n number of child.
CART can have max 2 number of child.


Classification Problem:

Purity of a split is checked by:
1)Entropy
2)Gini Index

Information Gain is used to select the feature to make a split.


Entropy: 

Formula H(C1): -(P+ log2P+) - (P- log2P-) For Binary Classification. 
Range is 0 to 1 where 1 is highly impure and 0 is pure
P+ is the probability of output being positive
P- is the probability of output being negative.

Gini Impurity:

Formula: 1 - Sum of i to n P^2, where P = (P+^2 + P-^2)
Range is 0 to 0.5 where 0 is pure where 0.5 is impure


Gini Impurity:

Generally faster to compute
Works well with small datasets
Often preferred in random forest algorithms
Tends to isolate the most frequent class in its own branch

Entropy:

Can be more sensitive to impurities
May create more balanced trees
Often used in ID3 algorithm
Can be slightly slower to compute than Gini

In practice, the choice between Gini impurity and entropy often doesn't make a significant difference in the performance of the decision tree. Many data scientists experiment with both and choose the one that gives slightly better results for their specific dataset.

Information Gain:

Information Gain is used to find the best feature to split.

Formula: H(S) - Sum of ( (|Sv| / |S|) * H(Sv))

Here H(S) is the entropy of the root feature.
Then Next part represent the sum of 
|Sv| is the total number of splits of the child Like for a binary category we have 5yes/3 No then Sv = 8
|S| is the total number of splits in root
H(Sv) is the entropy of the child
We will calculate this for every child of root node and depth of the tree can be n i.e. tree can be classified or split to n depth.

Prunning:

Prunning is used to avoid overfitting of data which happens when we create a decision tree with all possible splits. It is also called Low Bais and High Variance.

Two Types:
Post Prunning: First we create Decision Tree, then apply prunning. Usefull for small dataset.
Pre Prunning: Hyperparameter tuning while constructing Decision Tree.
Some of the features: max depth, max feature, criterrian, splitter etc. 


Regression Problem:

We use variance to decide the split.

We will do it for each feature to decide and same as classification problem we will apply this to root and its child.
Variance(root) = 1/n (Sum of 1 to n (y - Ybar(Average Y))^2)
here n is no. of total element.
For example we feature Year and it has 3,4,5,6,7 and we make a split with year<=3 so on left we have 1 element and for right we 4 elements.
Taking this example, if root is 4 then n is 5.
For its child on left n = 1 and for right child n = 4

After we calculate this for all root feature we will apply Varience Reduction:
Formula = Var(root) - Sum of 1 to n(Wi * Var child)
Now Wi for above example we will be:
For left child only 1 element is from total 5 elements so its weight will be 1/5.
For right child it has 4 elements out of 5 so its weight will be 4/5.
Root node with high variance reduction is selected.


Types of Weight Distribution techniques:

Here I/P and O/P means number of Inputs and Outputs

1) Uniform Distribution: Here value Wij where i is hidden layer and j is unit in hidden layer is selected from a uniform distribution whose min value is -1/root of I/P
and max is 1/ root of I/P.

2)Xavier/Glora Initialization:
  Two Types:  
            1) Xavier Normal initialization from normal distribution where Wij is equivalent to N(0,σ) where σ = Root of (2/input + output)
            2) Xavier Uniform also from normal distribution but range is min = - (root 6)/ root input+output max = root 6)/ root input+output

3) Kaiming He Initialization:
  Two Types:
            1) He Normal initializatio same as above mut different range  Wij is equivalent to N(0,σ) where σ = Root of (2/input)
            2) He Uniform same as above type but range is min = - (root 6)/ root input max = root 6)/ root input

Dropout:
Dropout is a regularization technique used in neural networks to prevent overfitting. Here's an overview of the key aspects of dropout:

Basic concept:
Randomly "drop out" (i.e., temporarily remove) a portion of neurons during training.

How it works:
During each training iteration, each neuron has a probability p of being temporarily disabled.
The network is forced to learn with a different subset of neurons each time.
(Or in training instead of disabling the neuron we can multiply its weight with p)

Implementation:
Training phase: Multiply each neuron's output by a binary mask (0 or 1) drawn from a Bernoulli distribution with probability p.
Inference phase: All neurons are active, but their outputs are scaled by p to maintain the expected output magnitude.

Benefits:
Reduces overfitting by preventing complex co-adaptations between neurons.
Acts as an ensemble method, effectively training many different networks.
Improves generalization by making the network more robust.




Random Forest Classifier:

Ensemble Technique: Ensemble learning is a machine learning technique that aggregates two or more learners (e.g. regression models, neural networks) in order to produce better predictions.

Types:
1) Bagging: N number of model are trained on subset of dataset parallely and then their output is combined to get the output. For classification Major Voting(Class that is selected by most model) and for regression Average is taken of all output.Example
    Bagging Meta-estimator.
    Random Forest.
    Extra Trees (Extremely Randomized Trees)
    Pasting.
    Bootstrap Aggregating (Bagging)
    Balanced Random Forest.
    Random Subspaces.
    Feature Bagging.
2) Boosting: N number of model are trainned sequentially. To first model entire dataset is passed then it makes prediction, the prediction that are incorrect are passed to the next weak learner with the dataset these cycle is repeated till a strong learner is produced by combining all weak learner.


Random Forest:

For each model in decision tree we provide a subset of dataset which is done using row sampling and feature sampling which means we only pass selected rows and columns to the model. Two or model might have same features and row.

Classification: Major Voting
Regression: Average of Output of all model

Why use Random Forest instead of Decision Tree?
Because Decision Tree leads to overfitting i.e low bais and high vairance.




Column Transformer:
Column transformer is used to combine preprocessing of different types of data like for numeric standardscalar and for categorical one hot encoding.
Example:

preprocessor = ColumnTransformer([
("OneHotEncoder",name_of_transformer_obj,name_of_columns),
("StandardScalar",name_of_transformer_obj,name_of_columns)
])



SVM (Support Vector Machine):

https://chatgpt.com/share/33cc8a55-888a-4327-ba38-d6b01d1632cd

It has two Types:
1) Support Vector Regressor
2) Support Vector Classifier


Distance below the plane or angle with the best fit line and point > 90 degree, distance will be negative.
Distance above the plane or angle with the hyperplane and point is less than 90 degree, distance will be positive.
Formula for best fit line = W(transpose)ofx + b = 0 
Formula for positive support vector = W(transpose)ofx + b = +1
Formula for negative support vector = W(transpose)ofx + b = -1
Cost Function = Min(w,b) ||w||/2  (||w|| is called magnitude of w and cost function should be maximized to increase the distance between the support vector)



Convolution Neural Network:

It is used for image processing.

Image types:
Gray Scale Image(Black and white Image): has 1 channel and is represented Length * width * No. Of Channel = 6*6*1
RGB Image(Color Image): has 3 channel. Example 6*6*3

1) Step is to Normalize image to get its value between 0 to 1. Which can be done by dividing value of each pixel by 255 because range of pixel is 0 to 255.
2) Then we perform convulution operation which is the process of extracting features from images which done putting up a matrix know as filter(There can be of many types and its values are derived from back propagation and filter can be smaller than image or of same size. It helps extract feature like veritcal or horizontal or circular edges etc) on top of the image and in multiped and added to each of the scalar of image to create a new matrix. Stride is used to define the movement of the filter like after 1st output by how much pixel it should be shifted to in any direction .See video to understand more.
3)This new matrix is smaller than the image which results in information loss so we use padding.
  Two Types:
  1) Zero Padding: We assign zero to the new zone
  2) Neighbour Padding: We assign the value of the nearest pixel or neighbour pixel.
To define the size of padding formula is
n-f+2p+1=x
N is size of original image
f is size of frame
p is padding
x is the size of the required output size

4) After applying filters we get their respective matrices on which we apply relu activation function to assign weights effective during backward propagation.
5) All the process till now it is called Convolutional layer and their can be n number of convonutional layers.
6) Min, Average, Max pooling:
Pooling is a technique used to extract more feature from the resultant matrices of the filter.
Here a pooling matrix is created, which applied on the resultant matrix and it creates a new matrix and value of each scalar in new matrix is the max value of the transposed matrices the resultant matrix and pooling matrix then pooling matrix is shifted by stride.
Same for min and average. 

It is called Location invariant.

7)Flattening means to convert all matrices into a single column vector. All matrices are appended one after another.
8)This output is passed to Fully connected neural network.
9) From input to flatting is called Feature Extraction, Then FCNN is called classification.

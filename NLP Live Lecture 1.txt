1. Tokenization

    Definition: The process of breaking down a text into smaller units called tokens (words, phrases, or symbols).
    Example: "Natural Language Processing" -> ["Natural", "Language", "Processing"]

2. Stop Words

    Definition: Commonly used words in a language that are often removed during preprocessing because they do not carry significant meaning.
    Example: "is", "and", "the"

3. Stemming

    Definition: The process of reducing words to their base or root form by removing suffixes.
    Example: "running" -> "run"

4. Lemmatization

    Definition: Similar to stemming, but it reduces words to their base or dictionary form (lemma), considering the context.
    Example: "better" -> "good", "running" -> "run".

5. Named Entity Recognition
    Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important           information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as        well as themes, topics, times, monetary values and percentages.

    In short categorizes the words is corpus.

6. Bag of Words
    BOW are used to represent sentences in a vector form.
    It is simple to implement.
    Does not give meaningful result i.e if we two sentences one sentence has not and other does not then the model will only show difference of vector.
    Working :- Create the vocabulary and use vocabulary as feature and if the word is present in document represent it with 1 else 0.

7. N Gram
    To get meaning full results from BOW, we use NGram
    N gram works by making combination of n words to create much difference between sentences.
    Count Vectorizer is used for N gram implimentation. Its parameter are as follows:
    Max_feature specify the top n words used in the dataset
    binary means if same word is repeated in same sentence it will represented by 1, if false it will show the count of word
    ngram specified the combination of words to created first argument is least no. of word combination and second is max no. of words to be extracted
    From the given example, it will create ngrams of aleast of two words and max of 10 words

8. TFIDF
    Link for TFIDF.
    https://builtin.com/articles/tf-idf
    TF = Number of times Term(Word) in a sentence / Total no. of words in that sentences
    IDF = Log e( No. of Sentences /  No. Of Sentences containing the word)


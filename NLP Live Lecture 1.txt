Working with NLP

1. Preprocessing of Data
2. Train and Test Split before modelling to prevent any data leakage
3. BOW or TFIDF
4. Model training


Perceptron is single layered  neural network used to solve binary classification problem.


1. Tokenization

    Definition: The process of breaking down a text into smaller units called tokens (words, phrases, or symbols).
    Example: "Natural Language Processing" -> ["Natural", "Language", "Processing"]

2. Stop Words

    Definition: Commonly used words in a language that are often removed during preprocessing because they do not carry significant meaning.
    Example: "is", "and", "the"

3. Stemming

    Definition: The process of reducing words to their base or root form by removing suffixes.
    Example: "running" -> "run"

4. Lemmatization

    Definition: Similar to stemming, but it reduces words to their base or dictionary form (lemma), considering the context.
    Example: "better" -> "good", "running" -> "run".

5. Named Entity Recognition
    Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important           information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as        well as themes, topics, times, monetary values and percentages.

    In short categorizes the words is corpus.

6. Bag of Words
    BOW are used to represent sentences in a vector form.
    It is simple to implement.
    Does not give meaningful result i.e if we two sentences one sentence has not and other does not then the model will only show difference of vector.
    Working :- Create the vocabulary and use vocabulary as feature and if the word is present in document represent it with 1 else 0.

7. N Gram
    To get meaning full results from BOW, we use NGram
    N gram works by making combination of n words to create much difference between sentences.
    Count Vectorizer is used for N gram implimentation. Its parameter are as follows:
    Max_feature specify the top n words used in the dataset
    binary means if same word is repeated in same sentence it will represented by 1, if false it will show the count of word
    ngram specified the combination of words to created first argument is least no. of word combination and second is max no. of words to be extracted
    From the given example, it will create ngrams of aleast of two words and max of 10 words

8. TFIDF
    Link for TFIDF.
    https://builtin.com/articles/tf-idf
    TF (Term Frequency) = Number of times Term(Word) in a sentence / Total no. of words in that sentences
    IDF (Inverse Document Frequency) = Log e( No. of Sentences /  No. Of Sentences containing the word

Example:
    1. Boy Good
    2. Girl Good
    3. Boy Girl Good

For Sentence 1:
TF of good = No. of times word appeared / Total no. of words
           = 1/2
TF of Boy = No. of times word appeared / Total no. of words
           = 1/2

For Sentence 3:
TF of good = No. of times word appeared / Total no. of words
           = 1/3
TF of Boy = No. of times word appeared / Total no. of words
           = 1/3
TF of Girl = No. of times word appeared / Total no. of words
           = 1/2

IDF for Good: good is all three sentences so IDF = log e(3/3) = 0
IDF for Boy : log e (3/2)
IDF for Girl: log e (3/2)

Final TFIDF = TF * IDF
    
                Good         Boy                Girl
Sentence 1:     1/2 * 0      1/2 * log e(3/2)   0*log e(3/2) 
Sentence 2:     1/2 * 0      0*log e(3/2)       1/2 * log e(3/2)
Sentence 3:     1/2 * 0      1/2 * log e(3/2)   1/2 * log e(3/2)

Advantage of TFIDF:

1. It can tell importance of each words.
2. Fixed size.

Disadvantage:

1. Out of vocabulary problem.


9. Word Embedding:
    term used for representation of words in text analysis, it is representation of words in real valued vector form that encodes the meaning of word and similar words are 
    closer to each other in terms of plan representation(Graph).

    Two Types of word Embedding technique:
    1. Count or Frequency based. Example :- BOW, OHE, TFIDF
    2. Deep learning trained model. Example:- Word2Vec:- CBOW(Continous Bag of Words), Skipgram

    Advantages:
    1. Does not generate Sparse Matrix, generates Dense Matrix.
    2. Out off vocabulary problem solved as new feature/word can be represented with 0.
    3. Semantic Information is getting captured.


10. CBOW(Continous Bag Of Words):
    CBOW is a neural network-based algorithm that predicts a target word given its surrounding context words.
    Works well with small dataset.
    CBOW Working:
    Let's take for Example a document: I am learning Deep Learning.
    And Suppose we are taking the window size of 3.
    So will the break the whole sentence using this window size.
    It uses ANN to generate the output.
    Softmax function is used.
    Middle word of the sentence will be considered output.
    Except the middle word other will act as Input.
    1. I am learning, Here 'am' is the Output.
    So, here we will the first we will apply one hot encoding to the sentence.
    Then as there are 2 words as input we will apply 2 vectors to the hidden layer as input.
    Here wieght applied for each node will equal to 5*3 where 5 is total no. of words (vocabulary) and 3 is the window size.
    The hidden layer is of size equal to window size for this example.
    A typical range for word embeddings is between 50 and 300 dimensions for small to moderate-sized datasets. For larger datasets, embeddings can go up to 1000 dimensions      or more.
    The output layer will be of size vocabulary i.e total no, of nodes in output layer is equal to total no. of words.
    The ouput generated will not be totally out off vocabulary (Read the chat given after intution for More info)

    Intution:
    Here, each word is considered as a feature for example lets take words Apple, elephant 
    And we will create CBOW for this 2 words based on features is_fruit or is_animal
    So for apple, vector will be[1, 0.01] this shows the relation with fruit and animal 1 represents fully related and 0.01 not related
    Therefore for above example, we will be generating this types of vectores and compare them with output and will apply loss function to it.

    More Detailed Answer: https://chatgpt.com/share/1e247421-80c5-470b-99dd-bc0e580da2a0


11. SkipGram:
    Skip-gram is used to predict the context word for a given target word.
    Good with large dataset.

    Working:
    Same as CBOW, the key difference will be the ouput of CBOW becomes input for Skipgram and input of CBOW becomes output of Skipgram.
    Or to put it simple:
    It will a take a word as input and will generate words related to the input.

    Detailed Answers: https://chatgpt.com/share/51b9bae5-e392-4576-9207-68bedfb70b3a

12. Average Word2Vec:
    Average Word2Vec is used in deep learning beacause Word2Vec just converts a word to vector but in real time analysis we have to analyze sentences.
    So solve this we use Average Word2Vec.
    Working:
    We obtain the word2vec of each word.
    Then for each row of each vector we calculate average and store it in a new vector.
    Example: 3 verctors A,B,C output D
    i in range(0,len(A)):
        (Ai+Bi+Ci)/3 = Di

13. To understand Working of BOW with Example link: https://chatgpt.com/share/6d98d575-3635-4acc-8cc2-1a565ce05f4a

14. Docker:

    Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that       have everything the software needs to run including libraries, system tools, code, and runtime.

15. Docker Image:
    1. It is the package containing all the image files of the required dependencies.
    2. It is layered architecture with os image as a base example linux.
    3. For understanding purpose consider it as program containing all the necessary information to run the application.

16. Container:
    1. Container is created when docker image file is executed.
    2. Container can be considered as the environment having all the necessary dependencies to run the application.
    3. For understanding purpose consider it as a process.

17. Docker And Virtual Machine:
    Docker: 
        Communicates with the host OS kernel so it doesn't need its own OS kernel.
        Only needs application layer not kernel level to communicate with hardware.
        Does not directly communicates with hardware.
        Because of this small in size.
        Starts and runs faster.
        Has compatibility issues for example a linux docker image will not work on windows version less than win 10 because OS kernel of win <10 does not support linux.
    Virtual Machine:
        Directly communicates with the hardware.
        Contains its own application layer and os kernel layer.
        Because of this large in size.
        Slower compared to docker.
        Can be installed on any OS.

18. Docker Commands:
    1.docker pull docker/img_name: Used to download/pull specified image.
    2.docker images: To check all the pulled imgaes.
    3.docker run -d -p port:port, port:port is host to container mapping, -d means detached, -p means port mapping: This command is used to run container.
    4.docker ps: to display information like container if, status, port all the running containers. And if it is running it will show the port no. on which it is running          0.0.0.0:80 means it is running on local
    host, 127.0.0.1:80, using this it can be accessed on browswer.
    5.docker stop container_id: to stop the container.
    6.docker image rm -f container_id: to delete the image
    7. docker build -t name . // This command creates the image name is the name of image and . means it finds the DockerFile in this folder
    8. docker tag old-name new-name //Used to change the name of the image
    9.docker push full-img-name

19. Creating A Docker Image:
    FROM python:3.8-alpine //From specifies the base layer or the base os 
    COPY . /app //This command '.' means from this current directory copy all files to '/app'
    WORKDIR /app 'This command changes the working directory to the current directory'
    RUN pip install -r requirements.txt // 'This command installs all the libraries specified in the requirements.txt file'
    CMD python app.py // This command launches the application

20. Steps to upload/push image
    1. Login to docker in cmd
    2. Rename your image to docker-usrname/img-name
    3. docker push full-img-name


21. Vanishing Gradient Problem:

    It occurs due to sigmoid activation function.
    Vanishing Gradient Problem occurs when in back propagation the weight updation is so small that it becomes insiginicant. (Wnew is approx of W old)
    
    Weight Updation Formula = W(new) = W(old) - Learning Rate(n of) (derivative of Loss/derivative of W(old))
    Now the derivative of a weight is dependent on activation function and 

    Derivative of a Sigmoid Function is always between 0 to 0.25.

    Now when we calculate weight of the beginnig point we have muliply values near 0.25 many times which in terms becomes insignicant.
    Sigmoid activation function can be used for small neural networks.

    Sigmoid is not zero centered it is centered at 0.5 which is not good because after we apply stanadard scalar to inputs there value will be between
    -1 to 1 centered to 0. Because of this weight updation will not be that good.
    Zero centered means the mean is zero.

    Softmax activation function is used for Multiclass classification problem
    Softmax formular = e^yi/Sum of k = 0 to n for e^yk

    In Relu Dead neuron may produce.
    To solve Dead neuron/Relu problem Parametric Relu and Leaky Relu is Used.
    Parametric Relu = max(const * x, x) this way we will never get 0.
    Leaky Relu = max(0.01*x, x)

22. Activation Function Return values:
    1. Sigmoid = 0 to 1  derivative = 0 to 0.25
    2. Tanh = -1 to 1    derivative = 0 to 1
    3. Relu = max(0, input) derivative = 0 or 1

23. Loss Funtion and Cost Function:

    Loss Funtion is a function that calculates difference between the predicted value and the absolute value.
    Cost Function is a a function calculates difference between the predicted value and the absolute value. for multiple records first calculation is done and then back         propagation is done.

    Back Propogation is not done by loss function, it just calculates the loss.

    Types in Regression:
    1) Mean Squared Error:
        Cost Function = (1/n)* Sum of yi - yihat where 0<=i<=n

        Do not use when dataset contains outliers.

    2) Mean Absolute Error:
        
        Function = |y-yhat|

        Use when dataset have outliers because it will not affect the calculation of classification line in classification problem.
        Creates a sub gradient which instead of curve like gradient discent it creates a V.

    3) Huber Loss:
        Combination of MSE and MAE
        When error is less than a specified hyper parameter use MSE else use MAE

    Types in Classification (Cross Entropy):

    1)Binary cross entropy uses sigmoid.
    2)Categorical Cross Entropy for multiclass regression uses softmax function which returns propability of all the classes Requires One Hot Encoding.
    3)Sparse Cross Entropy similar to categorical but instead of returning the probability of all classes returns the index of class with the highest probability.

24. Epochs and Iteration:
    
    Epochs: An epoch is one complete pass through the entire training dataset. It means that every unique sample in the training dataset has been used once to update the        model's weights. When training a model, multiple epochs are usually run to ensure that the model learns the patterns in the data effectively.
    For example, if you have a dataset with 1,000 samples and you run training for 10 epochs, it means that the model will go through the entire dataset 10 times.

    Iteration: An iteration refers to a single update of the model's parameters. The number of iterations depends on the batch size and the size of the dataset. During          each iteration, a batch of samples is processed, and the model weights are updated based on the gradient computed from that batch.
    For example, if you have a dataset with 1,000 samples and a batch size of 100, then it will take 10 iterations to complete 1 epoch.


25. Optimizers:

    Optimizers are the methods or algorithms used to update the weights and baisis of the model to minize the loss.

    Different Types:
    1)Gradient Descent
    2)Stochastic Gradient Descent
    3)Mini Batch SGD
    4)SGD with momentum
    5)Adagard and RMSPROP
    6)Adam Optimizer


    1) Gradient Descent works by taking all the data points at once and uses cost function. If we have 100 data points it will pass all 100 points to model, compute the            cost and then apply optimization method.

       It is resource intensive as it may require large amount of ram for large amount of data.

    2)Stochastic Gradient Descent: Here we will take only one data point at a time to train the model and optimizes the weight and bais.

      Solves the problem of resource as requires less resources.
      Distadvantages:
      1) Time Complexity- Requires more time.
      2) Convergence is slow.
      3) Introduces noise.(Doest not have smooth curve in gradient decent curve while solving)

    3) Mini Batch SGD:
       Here instead of one data point multiple data points are taken at once in model and cost function is calculated and the weights are optimized.
       Advantages: Improved Convergence Speed.
       Less time required and Efficient use of resources.

       Disadvantages: Noise Still Exists.

    4)SGD with momentum:
      Here it improves the Mini Batch SGD by smoothing the gradient descent curve, which improves convergence speed.
      It uses the concept of Exponential Weighted Average which stats that we use time and values to smoothening the gradient descent curve.
      The formula to calculate weight is Wt = Wt-1 - Learning rate of(derivative of Loss/ derivative Wt-1)
      Suppose for time t1,t2,t3,t4 we have values for a weigth a1,a2,a3,a4 respectively.    
      Vt1= a1 (Value of t1 is a1)
      So to smoothen the curve we will use Smoothing function which is Vt = Beta* Vt-1 + (1- Beta)*at
      where Beta controls the smoothening and it has value between 0 and 1. The smoothening more dependent on previous value.
      Example Beta = 0.95, V2 = (0.95 * V1) + (0.05*a2)

    5)AdaGrad (Adaptive Gradient Descent)
      Before this the learning rate was static, here learning rate is dynamically changed the learning rate is decreased as it reaches near to the global minimum.
      n' represents dynamic learning rate.
      n' = (n/root of(Alpha t + epsilon) 
      here, epsilon is very small value so dinominator does not become 0.
      Alpha t = Sum of i =1 to t(Derivative of loss/derivative of Wt)



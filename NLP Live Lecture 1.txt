Working with NLP

1. Preprocessing of Data
2. Train and Test Split before modelling to prevent any data leakage
3. BOW or TFIDF
4. Model training


Perceptron is single layered  neural network used to solve binary classification problem.


1. Tokenization

    Definition: The process of breaking down a text into smaller units called tokens (words, phrases, or symbols).
    Example: "Natural Language Processing" -> ["Natural", "Language", "Processing"]

2. Stop Words

    Definition: Commonly used words in a language that are often removed during preprocessing because they do not carry significant meaning.
    Example: "is", "and", "the"

3. Stemming

    Definition: The process of reducing words to their base or root form by removing suffixes.
    Example: "running" -> "run"

4. Lemmatization

    Definition: Similar to stemming, but it reduces words to their base or dictionary form (lemma), considering the context.
    Example: "better" -> "good", "running" -> "run".

5. Named Entity Recognition
    Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important           information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as        well as themes, topics, times, monetary values and percentages.

    In short categorizes the words is corpus.

6. Bag of Words
    BOW are used to represent sentences in a vector form.
    It is simple to implement.
    Does not give meaningful result i.e if we two sentences one sentence has not and other does not then the model will only show difference of vector.
    Working :- Create the vocabulary and use vocabulary as feature and if the word is present in document represent it with 1 else 0.

7. N Gram
    To get meaning full results from BOW, we use NGram
    N gram works by making combination of n words to create much difference between sentences.
    Count Vectorizer is used for N gram implimentation. Its parameter are as follows:
    Max_feature specify the top n words used in the dataset
    binary means if same word is repeated in same sentence it will represented by 1, if false it will show the count of word
    ngram specified the combination of words to created first argument is least no. of word combination and second is max no. of words to be extracted
    From the given example, it will create ngrams of aleast of two words and max of 10 words

8. TFIDF
    Link for TFIDF.
    https://builtin.com/articles/tf-idf
    TF (Term Frequency) = Number of times Term(Word) in a sentence / Total no. of words in that sentences
    IDF (Inverse Document Frequency) = Log e( No. of Sentences /  No. Of Sentences containing the word

Example:
    1. Boy Good
    2. Girl Good
    3. Boy Girl Good

For Sentence 1:
TF of good = No. of times word appeared / Total no. of words
           = 1/2
TF of Boy = No. of times word appeared / Total no. of words
           = 1/2

For Sentence 3:
TF of good = No. of times word appeared / Total no. of words
           = 1/3
TF of Boy = No. of times word appeared / Total no. of words
           = 1/3
TF of Girl = No. of times word appeared / Total no. of words
           = 1/2

IDF for Good: good is all three sentences so IDF = log e(3/3) = 0
IDF for Boy : log e (3/2)
IDF for Girl: log e (3/2)

Final TFIDF = TF * IDF
    
                Good         Boy                Girl
Sentence 1:     1/2 * 0      1/2 * log e(3/2)   0*log e(3/2) 
Sentence 2:     1/2 * 0      0*log e(3/2)       1/2 * log e(3/2)
Sentence 3:     1/2 * 0      1/2 * log e(3/2)   1/2 * log e(3/2)

Advantage of TFIDF:

1. It can tell importance of each words.
2. Fixed size.

Disadvantage:

1. Out of vocabulary problem.


9. Word Embedding:
    term used for representation of words in text analysis, it is representation of words in real valued vector form that encodes the meaning of word and similar words are 
    closer to each other in terms of plan representation(Graph).

    Two Types of word Embedding technique:
    1. Count or Frequency based. Example :- BOW, OHE, TFIDF
    2. Deep learning trained model. Example:- Word2Vec:- CBOW(Continous Bag of Words), Skipgram

    Advantages:
    1. Does not generate Sparse Matrix, generates Dense Matrix.
    2. Out off vocabulary problem solved as new feature/word can be represented with 0.
    3. Semantic Information is getting captured.


10. CBOW(Continous Bag Of Words):
    CBOW is a neural network-based algorithm that predicts a target word given its surrounding context words.
    Works well with small dataset.
    CBOW Working:
    Let's take for Example a document: I am learning Deep Learning.
    And Suppose we are taking the window size of 3.
    So will the break the whole sentence using this window size.
    It uses ANN to generate the output.
    Softmax function is used.
    Middle word of the sentence will be considered output.
    Except the middle word other will act as Input.
    1. I am learning, Here 'am' is the Output.
    So, here we will the first we will apply one hot encoding to the sentence.
    Then as there are 2 words as input we will apply 2 vectors to the hidden layer as input.
    Here wieght applied for each node will equal to 5*3 where 5 is total no. of words (vocabulary) and 3 is the window size.
    The hidden layer is of size equal to window size for this example.
    A typical range for word embeddings is between 50 and 300 dimensions for small to moderate-sized datasets. For larger datasets, embeddings can go up to 1000 dimensions      or more.
    The output layer will be of size vocabulary i.e total no, of nodes in output layer is equal to total no. of words.
    The ouput generated will not be totally out off vocabulary (Read the chat given after intution for More info)

    Intution:
    Here, each word is considered as a feature for example lets take words Apple, elephant 
    And we will create CBOW for this 2 words based on features is_fruit or is_animal
    So for apple, vector will be[1, 0.01] this shows the relation with fruit and animal 1 represents fully related and 0.01 not related
    Therefore for above example, we will be generating this types of vectores and compare them with output and will apply loss function to it.

    More Detailed Answer: https://chatgpt.com/share/1e247421-80c5-470b-99dd-bc0e580da2a0


11. SkipGram:
    Skip-gram is used to predict the context word for a given target word.
    Good with large dataset.

    Working:
    Same as CBOW, the key difference will be the ouput of CBOW becomes input for Skipgram and input of CBOW becomes output of Skipgram.
    Or to put it simple:
    It will a take a word as input and will generate words related to the input.

    Detailed Answers: https://chatgpt.com/share/51b9bae5-e392-4576-9207-68bedfb70b3a

12. Average Word2Vec:
    Average Word2Vec is used in deep learning beacause Word2Vec just converts a word to vector but in real time analysis we have to analyze sentences.
    So solve this we use Average Word2Vec.
    Working:
    We obtain the word2vec of each word.
    Then for each row of each vector we calculate average and store it in a new vector.
    Example: 3 verctors A,B,C output D
    i in range(0,len(A)):
        (Ai+Bi+Ci)/3 = Di

13. To understand Working of BOW with Example link: https://chatgpt.com/share/6d98d575-3635-4acc-8cc2-1a565ce05f4a

14. Docker:

    Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that       have everything the software needs to run including libraries, system tools, code, and runtime.

15. Docker Image:
    1. It is the package containing all the image files of the required dependencies.
    2. It is layered architecture with os image as a base example linux.
    3. For understanding purpose consider it as program containing all the necessary information to run the application.

16. Container:
    1. Container is created when docker image file is executed.
    2. Container can be considered as the environment having all the necessary dependencies to run the application.
    3. For understanding purpose consider it as a process.

17. Docker And Virtual Machine:
    Docker: 
        Communicates with the host OS kernel so it doesn't need its own OS kernel.
        Only needs application layer not kernel level to communicate with hardware.
        Does not directly communicates with hardware.
        Because of this small in size.
        Starts and runs faster.
        Has compatibility issues for example a linux docker image will not work on windows version less than win 10 because OS kernel of win <10 does not support linux.
    Virtual Machine:
        Directly communicates with the hardware.
        Contains its own application layer and os kernel layer.
        Because of this large in size.
        Slower compared to docker.
        Can be installed on any OS.

18. Docker Commands:
    1.docker pull docker/img_name: Used to download/pull specified image.
    2.docker images: To check all the pulled imgaes.
    3.docker run -d -p port:port, port:port is host to container mapping, -d means detached, -p means port mapping: This command is used to run container.
    4.docker ps: to display information like container if, status, port all the running containers. And if it is running it will show the port no. on which it is running          0.0.0.0:80 means it is running on local
    host, 127.0.0.1:80, using this it can be accessed on browswer.
    5.docker stop container_id: to stop the container.
    6.docker image rm -f container_id: to delete the image
    7. docker build -t name . // This command creates the image name is the name of image and . means it finds the DockerFile in this folder
    8. docker tag old-name new-name //Used to change the name of the image
    9.docker push full-img-name

19. Creating A Docker Image:
    FROM python:3.8-alpine //From specifies the base layer or the base os 
    COPY . /app //This command '.' means from this current directory copy all files to '/app'
    WORKDIR /app 'This command changes the working directory to the current directory'
    RUN pip install -r requirements.txt // 'This command installs all the libraries specified in the requirements.txt file'
    CMD python app.py // This command launches the application

20. Steps to upload/push image
    1. Login to docker in cmd
    2. Rename your image to docker-usrname/img-name
    3. docker push full-img-name


21. Vanishing Gradient Problem:

    It occurs due to sigmoid activation function.
    Vanishing Gradient Problem occurs when in back propagation the weight updation is so small that it becomes insiginicant. (Wnew is approx of W old)
    
    Weight Updation Formula = W(new) = W(old) - Learning Rate(n of) (derivative of Loss/derivative of W(old))
    Now the derivative of a weight is dependent on activation function and 

    Derivative of a Sigmoid Function is always between 0 to 0.25.

    Now when we calculate weight of the beginnig point we have muliply values near 0.25 many times which in terms becomes insignicant.
    Sigmoid activation function can be used for small neural networks.

    Sigmoid is not zero centered it is centered at 0.5 which is not good because after we apply stanadard scalar to inputs there value will be between
    -1 to 1 centered to 0. Because of this weight updation will not be that good.
    Zero centered means the mean is zero.

    In Relu Dead neuron may produce.
    To solve Dead neuron/Relu problem Parametric Relu and Leaky Relu is Used.
    Parametric Relu = max(const * x, x) this way we will never get 0.
    Leaky Relu = max(0.01*x, x)

22. Activation Function Return values:
    1. Sigmoid = 0 to 1  derivative = 0 to 0.25
    2. Tanh = -1 to 1    derivative = 0 to 1
    3. Relu = max(0, input) derivative = 0 or 1



1. Tokenization

    Definition: The process of breaking down a text into smaller units called tokens (words, phrases, or symbols).
    Example: "Natural Language Processing" -> ["Natural", "Language", "Processing"]

2. Stop Words

    Definition: Commonly used words in a language that are often removed during preprocessing because they do not carry significant meaning.
    Example: "is", "and", "the"

3. Stemming

    Definition: The process of reducing words to their base or root form by removing suffixes.
    Example: "running" -> "run"

4. Lemmatization

    Definition: Similar to stemming, but it reduces words to their base or dictionary form (lemma), considering the context.
    Example: "better" -> "good", "running" -> "run".

5. Named Entity Recognition
    Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important           information in text known as named entities. Named entities refer to the key subjects of a piece of text, such as names, locations, companies, events and products, as        well as themes, topics, times, monetary values and percentages.

    In short categorizes the words is corpus.

6. Bag of Words
    BOW are used to represent sentences in a vector form.
    It is simple to implement.
    Does not give meaningful result i.e if we two sentences one sentence has not and other does not then the model will only show difference of vector.
7. 
    

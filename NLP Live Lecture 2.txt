1. Corpus means a paragraph.
2. Document can be defined individual data points in a datasets or to be put simply we can say sentences.
3. Vocabulary no of unique words found in corpus.

Feed-Forward Neural Network
A Feed-Forward Neural Network (FFNN) is one of the simplest forms of artificial neural networks. It consists of multiple layers of neurons, where the information moves in one direction—from the input layer, through the hidden layers, and finally to the output layer.

Linear transformation is Multiplying inputs with Weight and adding a bias​.
z1=xW1+b1

Audio Classification Project:
IPython.display is used to play audio file and it takes file name and sample rate as arguments.
Librosa.display.waveplot(data,sampler_rate) is used to display graph.
Sample rate is the number of samples per second that are taken of a waveform to create a discete digital signal.
Audio sampling is the process of transforming a musical source into a digital file.
Librosa converts sterio channel to mono.
Librosa returns 2 Things sample rate and data.
Data are the wavepoints which can represented as an array of interger and it is normalized.
ANN Vs RNN
ANN cannot handle sequential data where position of an element is important for example if we pass it a sentence we convert it into a vector using BOW,Word2Vec which causes the information loss about the each words position.
Example when use google translate if have to process word sequentially to generate valid translations.


Difference Between ANN and Simple RNN:
ANN does not retain the information and all the inputs are passed in one timestamp i.e in one go.
In Simple RNN, its different, Every node in hidden layer contains a self loop or feedback loop and loop to other nodes in the hidden layer. When One node generates it output, this output is passed to itself and to other nodes. By doing this Simple RNN can retain previous information. Also inputs are passed once every time stamp.

Problems in Simple RNN:
1) When sentences has many words and has dependencies on prevous words it can not capture it well.
2) Also Vanishing gradiend problem occurs when no. of words is large as we have to calculate derivate for all and which may lead to weight updation at t1 to be near zero which means it does not contribute much in computation


LSTM: - 

Wieghts and baisis are called trainable parameters.

LSTM has 3 parts Forget Gate, Input Gate and Candidate memory and Output gate.
1)Let's define the notation first:

    x_t: input at time t
    h_t-1: hidden state at time t-1
    c_t-1: cell state at time t-1
    W: weight matrices
    b: bias vectors
    σ: sigmoid function
    tanh: hyperbolic tangent function
    ⊙: element-wise multiplication (Hadamard product)

    Input Gate: The input gate determines what new information will be stored in the cell state.

    Formula:
    i_t = σ(W_i · [h_t-1, x_t] + b_i)

    Forget Gate: The forget gate decides what information to discard from the cell state.

Formula:
f_t = σ(W_f · [h_t-1, x_t] + b_f)

    Output Gate: The output gate controls what information from the cell state will be output.

Formula:
o_t = σ(W_o · [h_t-1, x_t] + b_o)

Additionally, there are two more important components:

    Candidate Cell State: This is the new candidate values that could be added to the cell state.

Formula:
c̃_t = tanh(W_c · [h_t-1, x_t] + b_c)

    Cell State Update: The new cell state is calculated using the forget gate, input gate, and candidate cell state.

Formula:
c_t = f_t ⊙ c_t-1 + i_t ⊙ c̃_t

    Hidden State Update: The new hidden state is calculated using the output gate and the updated cell state.

Formula:
h_t = o_t ⊙ tanh(c_t)

In these formulas:

    W_i, W_f, W_o, and W_c are weight matrices for the input gate, forget gate, output gate, and candidate cell state respectively.
    b_i, b_f, b_o, and b_c are the corresponding bias vectors.
    [h_t-1, x_t] represents the concatenation of the previous hidden state and the current input.

These formulas work together to allow the LSTM to selectively remember or forget information over long sequences, addressing the vanishing gradient problem that affects simple RNNs.

The key strengths of this architecture are:

    The forget gate allows the network to reset its state when needed.
    The input gate allows selective updates to the cell state.
    The output gate allows the cell state to affect the hidden state only when needed.

2) PeepHole Connections in LSTM occurs in LSTM varient when Candidate memory is provided as input to all gates i.e. Forget gate, Input Gate, Ouput Gate.
3) Basically different types of LSTM are determined by the way we candidate memory or output put of gates like forgot gate to other gates.


2)GRU

GRU has only 1 memory cell

Certainly. The Gated Recurrent Unit (GRU) is a simplified version of the LSTM, with fewer parameters and gates. GRU combines the forget and input gates into a single "update gate" and merges the cell state and hidden state. Here are the formulas for GRU:
Let's first define the notation:

x_t: input at time t
h_t-1: hidden state at time t-1
W: weight matrices
b: bias vectors
σ: sigmoid function
tanh: hyperbolic tangent function
⊙: element-wise multiplication (Hadamard product)

Now, let's look at the components of GRU:

Update Gate:
The update gate decides how much of the past information (from previous time steps) needs to be passed along to the future. It acts similarly to the forget and input gates of an LSTM.

Formula:
z_t = σ(W_z · [h_t-1, x_t] + b_z)

Reset Gate:
The reset gate determines how much of the past information to forget.

Formula:
r_t = σ(W_r · [h_t-1, x_t] + b_r)

Candidate Hidden State:
This is the new candidate hidden state, computed using the reset gate.

Formula:
h̃_t = tanh(W · [r_t ⊙ h_t-1, x_t] + b)

Hidden State Update:
The new hidden state is a linear interpolation between the previous hidden state and the candidate hidden state, using the update gate.

Formula:
h_t = (1 - z_t) ⊙ h_t-1 + z_t ⊙ h̃_t
In these formulas:

W_z and W_r are weight matrices for the update gate and reset gate respectively.
W is the weight matrix for the candidate hidden state.
b_z, b_r, and b are the corresponding bias vectors.
[h_t-1, x_t] represents the concatenation of the previous hidden state and the current input.

Key points about GRU:

The update gate z_t determines how much of the previous hidden state to keep.
The reset gate r_t allows the model to forget past information by setting values close to 0.
The candidate hidden state h̃_t is computed using the reset gate, allowing the model to drop irrelevant information.
The final hidden state h_t is a mixture of the previous hidden state and the candidate hidden state, controlled by the update gate.

Compared to LSTM:

GRU has fewer parameters, which can make it faster to train and less prone to overfitting on smaller datasets.
GRU combines the cell state and hidden state, while LSTM keeps them separate.
GRU uses two gates (update and reset) instead of three (input, forget, output) in LSTM.

In practice, both LSTM and GRU can perform well, and the choice between them often depends on the specific task and dataset. GRU might be preferred for smaller datasets or when computational efficiency is a concern, while LSTM might have an edge in modeling longer-term dependencies in some cases.

3) Bidirecitional RNN: 
A Bidirectional Recurrent Neural Network (BiRNN) is a type of RNN that processes data in both forward and backward directions with two separate hidden layers. This allows the network to have information from both past and future states, providing more context for making predictions. Here’s a detailed explanation along with an example:
Architecture

    Input Layer: Sequence of data points (e.g., words in a sentence).
    Forward RNN Layer: Processes the input sequence from the first to the last element.
    Backward RNN Layer: Processes the input sequence from the last to the first element.
    Concatenation or Summation Layer: Combines the outputs from the forward and backward layers.
    Output Layer: Produces the final output based on the combined context.

Forward and Backward Passes

    Forward Pass: ht→=σ(Wih⋅xt+Whh⋅ht−1→+bh)ht​

​=σ(Wih​⋅xt​+Whh​⋅ht−1​+bh​)
ht→ht​ : Hidden state at time tt in the forward direction
    WihWih​: Input-to-hidden weights
    WhhWhh​: Hidden-to-hidden weights
    xtxt​: Input at time tt
    σσ: Activation function
    bhbh​: Bias term

Backward Pass: ht←=σ(Wih⋅xt+Whh⋅ht+1←+bh)ht​
​=σ(Wih​⋅xt​+Whh​⋅ht+1​+bh​)    
ht←ht​: Hidden state at time tt in the backward direction

Combining: The hidden states from the forward and backward passes can be concatenated or summed:

    Concatenation: ht=[ht→,ht←]ht​=[ht​​,ht​]
Summation: ht=ht→+ht←ht​=ht​​+ht​
Example

Let's consider a simple example of a BiRNN applied to a sequence of words in a sentence for a Named Entity Recognition (NER) task.

Sentence: "John lives in New York"

    Input Layer: The words are first converted into word embeddings (vectors).
        x1​ = embedding("John")
        x2​ = embedding("lives")
        x3​ = embedding("in")
        x4​ = embedding("New")
        x5​ = embedding("York")

    Forward RNN Layer:
        Processes the sentence from "John" to "York".
        Computes forward hidden states h1→,h2→,h3→,h4→,h5→h1​​,h2​​,h3​​,h4​​,h5​

Backward RNN Layer:

    Processes the sentence from "York" to "John".
    Computes backward hidden states h1←,h2←,h3←,h4←,h5←h1​​,h2​​,h3​​,h4​,h5​  ​.

Combining:

    For each time step, combine the forward and backward hidden states.
    For instance, at time step 1:
        h1=[h1→,h1←]h1​=[h1​​,h1​]

Output Layer:

    Use the combined hidden states to predict the entity tags for each word.
    For instance, the output could be:
        "John" -> "PERSON"
        "lives" -> "O" (no entity)
        "in" -> "O"
        "New" -> "LOCATION"
        "York" -> "LOCATION"


4) Encoder And Decoder (Seq to Seq Architecture):

Encoder encodes the text into context Vector using LSTM and passes this context vector Decoder.
Ct(Long memory) and ht(Short Memory) are both together consider Context Vector which represents entire sentence.
Decoder is also LSTM which generates One word at a time and passes it back it self to generate the whole sentence.
Bleu Score is used to measure the performance of Seq to Seq Models.
Seq2Seq's performance decrease as the length of sentence increases.

Encoder And Decoder Architecture takes input in sequence


5) Transformer (Encoder decoder Sequence)

Transformer are deep learning models that use self-attention mechanism to analyze and process natural language data.
Encoder does not work or take input in sequences instead batch of words are taken as inputs.
Self attention is also called scaled dot product attention.

Contextual Vector is vector that defines the attention to a token based on its relation with all other token.

Self Attention Working:

1)Token Embedding: Convert token into vectors.

2)Linear Transformation: Multiply the vectors with Matrices Wq, Wk and Wv to Obtain Query, Key, and Value.
    Query (Q):
    Intuitive: A question or search term; what you're looking for, or current term for which we want find attention.
    Formal: A learned vector representation of the current token's "request" for information.
    Key (K):
    Intuitive: A label or identifier for each piece of information. It is multiplied with query to generate the value.
    Formal: A learned vector representation of each token encoding its relevance to potential queries.
    Value (V):
    Intuitive: The actual content or information associated with each key.
    Formal: A learned vector representation of each token containing its informational content.
    It has initial value of the embedding.

3)Compute The score: Multiply Q with Ki (0<i<n where n is number of tokens) here one vector is transposed i.e. To convert it rows into columns and columns into rows.

4)Scale: We scale the score by dividing it with root of (dimension). It is done for all tokens and their scores.
    Advantages: Avoids Gradient Explode(The gradient explode problem essentially means that the weights in the neural network become too large.This problem typically occurs     in deep networks or RNNs with long sequences. As gradients are propagated backwards through many layers or time steps, they can accumulate and grow exponentially.)
    Avoids Softmax Saturation(Most probablity going to one token)


5)Apply Softmax which gives Attention weights for each token. 
For example If the sentences is "The Cat Sat" Then we will have Attention Weights for the = softmax([Scaled score of the with respect to all tokens]) = softmax([0,1,1]) 0 = Score with the and the, 1= score with the and cat, last for score with the and sat.

6)Weights sum of values:
We multiply attention weights with the value to get Contextual Vector.
Context Vector For "The" = Attention Weighti * Valuei i.e In [Attention Weight first scaler is for The and The so we multiply Atention Weight(1) * Value(the) , Second scalar is for the and cat so Attention Weight(2) , Value of cat same for the last]


6)Weighted Sum: of values: Here 

For basic understanding: 
http://jalammar.github.io/illustrated-transformer/
For Advance topic: 
https://chatgpt.com/share/5a78a857-e9c4-4d00-991a-40d4d1501331


Types of Positional Encoding:

1)Sinusoidal Position Encoding
2)Learned Positional Encoding










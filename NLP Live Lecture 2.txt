1. Corpus means a paragraph.
2. Document can be defined individual data points in a datasets or to be put simply we can say sentences.
3. Vocabulary no of unique words found in corpus.

Audio Classification Project:
IPython.display is used to play audio file and it takes file name and sample rate as arguments.
Librosa.display.waveplot(data,sampler_rate) is used to display graph.
Sample rate is the number of samples per second that are taken of a waveform to create a discete digital signal.
Audio sampling is the process of transforming a musical source into a digital file.
Librosa converts sterio channel to mono.
Librosa returns 2 Things sample rate and data.
Data are the wavepoints which can represented as an array of interger and it is normalized.
ANN Vs RNN
ANN cannot handle sequential data where position of an element is important for example if we pass it a sentence we convert it into a vector using BOW,Word2Vec which causes the information loss about the each words position.
Example when use google translate if have to process word sequentially to generate valid translations.


Difference Between ANN and Simple RNN:
ANN does not retain the information and all the inputs are passed in one timestamp i.e in one go.
In Simple RNN, its different, Every node in hidden layer contains a self loop or feedback loop and loop to other nodes in the hidden layer. When One node generates it output, this output is passed to itself and to other nodes. By doing this Simple RNN can retain previous information. Also inputs are passed once every time stamp.

Problems in Simple RNN:
1) When sentences has many words and has dependencies on prevous words it can not capture it well.
2) Also Vanishing gradiend problem occurs when no. of words is large as we have to calculate derivate for all and which may lead to weight updation at t1 to be near zero which means it does not contribute much in computation


LSTM: - 

Wieghts and baisis are called trainable parameters.

LSTM has 3 parts Forget Gate, Input Gate and Candidate memory and Output gate.
1)Let's define the notation first:

    x_t: input at time t
    h_t-1: hidden state at time t-1
    c_t-1: cell state at time t-1
    W: weight matrices
    b: bias vectors
    σ: sigmoid function
    tanh: hyperbolic tangent function
    ⊙: element-wise multiplication (Hadamard product)

    Input Gate: The input gate determines what new information will be stored in the cell state.

    Formula:
    i_t = σ(W_i · [h_t-1, x_t] + b_i)

    Forget Gate: The forget gate decides what information to discard from the cell state.

Formula:
f_t = σ(W_f · [h_t-1, x_t] + b_f)

    Output Gate: The output gate controls what information from the cell state will be output.

Formula:
o_t = σ(W_o · [h_t-1, x_t] + b_o)

Additionally, there are two more important components:

    Candidate Cell State: This is the new candidate values that could be added to the cell state.

Formula:
c̃_t = tanh(W_c · [h_t-1, x_t] + b_c)

    Cell State Update: The new cell state is calculated using the forget gate, input gate, and candidate cell state.

Formula:
c_t = f_t ⊙ c_t-1 + i_t ⊙ c̃_t

    Hidden State Update: The new hidden state is calculated using the output gate and the updated cell state.

Formula:
h_t = o_t ⊙ tanh(c_t)

In these formulas:

    W_i, W_f, W_o, and W_c are weight matrices for the input gate, forget gate, output gate, and candidate cell state respectively.
    b_i, b_f, b_o, and b_c are the corresponding bias vectors.
    [h_t-1, x_t] represents the concatenation of the previous hidden state and the current input.

These formulas work together to allow the LSTM to selectively remember or forget information over long sequences, addressing the vanishing gradient problem that affects simple RNNs.

The key strengths of this architecture are:

    The forget gate allows the network to reset its state when needed.
    The input gate allows selective updates to the cell state.
    The output gate allows the cell state to affect the hidden state only when needed.

2) PeepHole Connections in LSTM occurs in LSTM varient when Candidate memory is provided as input to all gates i.e. Forget gate, Input Gate, Ouput Gate.
3) Basically different types of LSTM are determined by the way we candidate memory or output put of gates like forgot gate to other gates.


2)GRU

GRU has only 1 memory cell

Certainly. The Gated Recurrent Unit (GRU) is a simplified version of the LSTM, with fewer parameters and gates. GRU combines the forget and input gates into a single "update gate" and merges the cell state and hidden state. Here are the formulas for GRU:
Let's first define the notation:

x_t: input at time t
h_t-1: hidden state at time t-1
W: weight matrices
b: bias vectors
σ: sigmoid function
tanh: hyperbolic tangent function
⊙: element-wise multiplication (Hadamard product)

Now, let's look at the components of GRU:

Update Gate:
The update gate decides how much of the past information (from previous time steps) needs to be passed along to the future. It acts similarly to the forget and input gates of an LSTM.

Formula:
z_t = σ(W_z · [h_t-1, x_t] + b_z)

Reset Gate:
The reset gate determines how much of the past information to forget.

Formula:
r_t = σ(W_r · [h_t-1, x_t] + b_r)

Candidate Hidden State:
This is the new candidate hidden state, computed using the reset gate.

Formula:
h̃_t = tanh(W · [r_t ⊙ h_t-1, x_t] + b)

Hidden State Update:
The new hidden state is a linear interpolation between the previous hidden state and the candidate hidden state, using the update gate.

Formula:
h_t = (1 - z_t) ⊙ h_t-1 + z_t ⊙ h̃_t
In these formulas:

W_z and W_r are weight matrices for the update gate and reset gate respectively.
W is the weight matrix for the candidate hidden state.
b_z, b_r, and b are the corresponding bias vectors.
[h_t-1, x_t] represents the concatenation of the previous hidden state and the current input.

Key points about GRU:

The update gate z_t determines how much of the previous hidden state to keep.
The reset gate r_t allows the model to forget past information by setting values close to 0.
The candidate hidden state h̃_t is computed using the reset gate, allowing the model to drop irrelevant information.
The final hidden state h_t is a mixture of the previous hidden state and the candidate hidden state, controlled by the update gate.

Compared to LSTM:

GRU has fewer parameters, which can make it faster to train and less prone to overfitting on smaller datasets.
GRU combines the cell state and hidden state, while LSTM keeps them separate.
GRU uses two gates (update and reset) instead of three (input, forget, output) in LSTM.

In practice, both LSTM and GRU can perform well, and the choice between them often depends on the specific task and dataset. GRU might be preferred for smaller datasets or when computational efficiency is a concern, while LSTM might have an edge in modeling longer-term dependencies in some cases.

3) Bidirecitional RNN: 
A Bidirectional Recurrent Neural Network (BiRNN) is a type of RNN that processes data in both forward and backward directions with two separate hidden layers. This allows the network to have information from both past and future states, providing more context for making predictions. Here’s a detailed explanation along with an example:
Architecture

    Input Layer: Sequence of data points (e.g., words in a sentence).
    Forward RNN Layer: Processes the input sequence from the first to the last element.
    Backward RNN Layer: Processes the input sequence from the last to the first element.
    Concatenation or Summation Layer: Combines the outputs from the forward and backward layers.
    Output Layer: Produces the final output based on the combined context.

Forward and Backward Passes

    Forward Pass: ht→=σ(Wih⋅xt+Whh⋅ht−1→+bh)ht​

​=σ(Wih​⋅xt​+Whh​⋅ht−1​+bh​)
ht→ht​ : Hidden state at time tt in the forward direction
    WihWih​: Input-to-hidden weights
    WhhWhh​: Hidden-to-hidden weights
    xtxt​: Input at time tt
    σσ: Activation function
    bhbh​: Bias term

Backward Pass: ht←=σ(Wih⋅xt+Whh⋅ht+1←+bh)ht​
​=σ(Wih​⋅xt​+Whh​⋅ht+1​+bh​)    
ht←ht​: Hidden state at time tt in the backward direction

Combining: The hidden states from the forward and backward passes can be concatenated or summed:

    Concatenation: ht=[ht→,ht←]ht​=[ht​​,ht​]
Summation: ht=ht→+ht←ht​=ht​​+ht​
Example

Let's consider a simple example of a BiRNN applied to a sequence of words in a sentence for a Named Entity Recognition (NER) task.

Sentence: "John lives in New York"

    Input Layer: The words are first converted into word embeddings (vectors).
        x1​ = embedding("John")
        x2​ = embedding("lives")
        x3​ = embedding("in")
        x4​ = embedding("New")
        x5​ = embedding("York")

    Forward RNN Layer:
        Processes the sentence from "John" to "York".
        Computes forward hidden states h1→,h2→,h3→,h4→,h5→h1​​,h2​​,h3​​,h4​​,h5​

Backward RNN Layer:

    Processes the sentence from "York" to "John".
    Computes backward hidden states h1←,h2←,h3←,h4←,h5←h1​​,h2​​,h3​​,h4​,h5​  ​.

Combining:

    For each time step, combine the forward and backward hidden states.
    For instance, at time step 1:
        h1=[h1→,h1←]h1​=[h1​​,h1​]

Output Layer:

    Use the combined hidden states to predict the entity tags for each word.
    For instance, the output could be:
        "John" -> "PERSON"
        "lives" -> "O" (no entity)
        "in" -> "O"
        "New" -> "LOCATION"
        "York" -> "LOCATION"




